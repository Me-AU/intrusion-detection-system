{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jG3Wj4eT5Ly"
      },
      "source": [
        "# Face Recog and Biometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install ngrok opencv-python-headless deepface mtcnn tf-keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI2S_FxHajXr"
      },
      "source": [
        "## Preproc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DtU9S7jWddhn"
      },
      "outputs": [],
      "source": [
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IMfs9lOzakiE"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply histogram equalization for better contrast\n",
        "    enhanced = cv2.equalizeHist(gray)\n",
        "\n",
        "    # Convert back to BGR (optional for color-based models)\n",
        "    enhanced = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    # Save the enhanced image\n",
        "    cv2.imwrite(\"enhanced_\" + image_path, enhanced)\n",
        "    return \"enhanced_\" + image_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3BiRZY4Tz9z"
      },
      "source": [
        "## DeepFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQxtl1Z4XDxd",
        "outputId": "aaa0cf71-aeb9-4d68-e035-828c80220042"
      },
      "outputs": [],
      "source": [
        "from deepface import DeepFace\n",
        "from mtcnn import MTCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8CNSLkr8eAKL"
      },
      "outputs": [],
      "source": [
        "def extract_faces(image_path, verbose=False):\n",
        "    \"\"\"\n",
        "    Detects and extracts all faces from an image.\n",
        "    Returns a list of cropped face images.\n",
        "    \"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Initialize MTCNN detector\n",
        "    detector = MTCNN()\n",
        "    detections = detector.detect_faces(image_rgb)\n",
        "\n",
        "    faces = []\n",
        "    for detection in detections:\n",
        "        x, y, width, height = detection['box']\n",
        "        # Crop the face\n",
        "        face = image_rgb[y:y+height, x:x+width]\n",
        "        faces.append(face)\n",
        "    if verbose:\n",
        "        print(f\"Detected {len(faces)} face(s) in {image_path}\")\n",
        "    return faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gDVj10CAcg0A"
      },
      "outputs": [],
      "source": [
        "# def verify_faces(image1_path, image2_path):\n",
        "#     \"\"\"\n",
        "#     Compares each detected face in one image to each detected face in another image.\n",
        "#     \"\"\"\n",
        "#     # Extract faces from both images\n",
        "#     faces1 = extract_faces(image1_path)\n",
        "#     faces2 = extract_faces(image2_path)\n",
        "\n",
        "#     if len(faces1) == 0:\n",
        "#         print(f\"No faces detected in {image1_path}\")\n",
        "#         return\n",
        "#     if len(faces2) == 0:\n",
        "#         print(f\"No faces detected in {image2_path}\")\n",
        "#         return\n",
        "\n",
        "#     print(f\"Detected {len(faces1)} face(s) in {image1_path}\")\n",
        "#     print(f\"Detected {len(faces2)} face(s) in {image2_path}\")\n",
        "\n",
        "#     # Compare each face in image1 with each face in image2\n",
        "#     for i, face1 in enumerate(faces1):\n",
        "#         for j, face2 in enumerate(faces2):\n",
        "#             try:\n",
        "#                 # Save temporary cropped face images for DeepFace\n",
        "#                 face1_path = f\"temp_face1_{i}.jpg\"\n",
        "#                 face2_path = f\"temp_face2_{j}.jpg\"\n",
        "#                 cv2.imwrite(face1_path, cv2.cvtColor(face1, cv2.COLOR_RGB2BGR))\n",
        "#                 cv2.imwrite(face2_path, cv2.cvtColor(face2, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "#                 # Perform verification\n",
        "#                 result = DeepFace.verify(face1_path, face2_path, detector_backend=\"mtcnn\", enforce_detection=False)\n",
        "#                 print(f\"Comparison between Face {i+1} in Image1 and Face {j+1} in Image2: {result}\")\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error comparing Face {i+1} in Image1 and Face {j+1} in Image2: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "roosE4d-fmkf"
      },
      "outputs": [],
      "source": [
        "def verify_against_allowed(allowed_images, unknown_images, verbose=False):\n",
        "    \"\"\"\n",
        "    Verifies if any unknown face matches any face from the allowed list.\n",
        "\n",
        "    Args:\n",
        "        allowed_images (list): List of file paths for images of allowed people.\n",
        "        unknown_images (list): List of file paths for images of unknown people.\n",
        "        verbose (bool): If True, prints detailed logs.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if any match is found, False otherwise.\n",
        "    \"\"\"\n",
        "    # Extract all faces from allowed images\n",
        "    allowed_faces = []\n",
        "    for image_path in allowed_images:\n",
        "        faces = extract_faces(image_path, verbose=verbose)\n",
        "        allowed_faces.extend(faces)\n",
        "\n",
        "    if len(allowed_faces) == 0:\n",
        "        if verbose:\n",
        "            print(\"No faces detected in allowed images.\")\n",
        "        return False\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Total detected faces in allowed images: {len(allowed_faces)}\")\n",
        "\n",
        "    # Extract all faces from unknown images\n",
        "    unknown_faces = []\n",
        "    for image_path in unknown_images:\n",
        "        faces = extract_faces(image_path, verbose=verbose)\n",
        "        unknown_faces.extend(faces)\n",
        "\n",
        "    if len(unknown_faces) == 0:\n",
        "        if verbose:\n",
        "            print(\"No faces detected in unknown images.\")\n",
        "        return False\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Total detected faces in unknown images: {len(unknown_faces)}\")\n",
        "\n",
        "    # Compare each unknown face against each allowed face\n",
        "    for i, unknown_face in enumerate(unknown_faces):\n",
        "        for j, allowed_face in enumerate(allowed_faces):\n",
        "            try:\n",
        "                # Save temporary cropped face images for DeepFace\n",
        "                unknown_face_path = f\"temp_unknown_face_{i}.jpg\"\n",
        "                allowed_face_path = f\"temp_allowed_face_{j}.jpg\"\n",
        "                cv2.imwrite(unknown_face_path, cv2.cvtColor(unknown_face, cv2.COLOR_RGB2BGR))\n",
        "                cv2.imwrite(allowed_face_path, cv2.cvtColor(allowed_face, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                # Perform verification\n",
        "                result = DeepFace.verify(unknown_face_path, allowed_face_path, detector_backend=\"mtcnn\", enforce_detection=False)\n",
        "                if verbose:\n",
        "                    print(f\"Comparison between Unknown Face {i+1} and Allowed Face {j+1}: {result}\")\n",
        "                if result['verified']:\n",
        "                    if verbose:\n",
        "                        print(f\"Match found: Unknown Face {i+1} matches Allowed Face {j+1}\")\n",
        "                    return True\n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"Error comparing Unknown Face {i+1} and Allowed Face {j+1}: {e}\")\n",
        "\n",
        "    if verbose:\n",
        "        print(\"No match found between unknown and allowed faces.\")\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7KhY99hSU1D",
        "outputId": "0db07830-5763-4f28-b7c3-1afd0fea57d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected 2 face(s) in low2_close.jpg\n",
            "Total detected faces in allowed images: 2\n",
            "Detected 1 face(s) in low1_far_tilt.jpg\n",
            "Total detected faces in unknown images: 1\n",
            "Comparison between Unknown Face 1 and Allowed Face 1: {'verified': True, 'distance': 0.3781767691793787, 'threshold': 0.68, 'model': 'VGG-Face', 'detector_backend': 'mtcnn', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 32, 'h': 45, 'left_eye': None, 'right_eye': None}, 'img2': {'x': 1, 'y': 0, 'w': 37, 'h': 51, 'left_eye': (29, 20), 'right_eye': (10, 19)}}, 'time': 4.71}\n",
            "Match found: Unknown Face 1 matches Allowed Face 1\n",
            "Verification Result: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Action: race: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analysis:  [{'emotion': {'angry': 0.007898546027718112, 'disgust': 1.2873186880036155e-14, 'fear': 4.621165618300438, 'happy': 0.0002979694500027108, 'sad': 5.572805553674698, 'surprise': 0.003523028499330394, 'neutral': 89.79431390762329}, 'dominant_emotion': 'neutral', 'region': {'x': 163, 'y': 138, 'w': 34, 'h': 45, 'left_eye': (194, 155), 'right_eye': (182, 155)}, 'face_confidence': 0.96, 'age': 28, 'gender': {'Woman': 12.587174773216248, 'Man': 87.41282224655151}, 'dominant_gender': 'Man', 'race': {'asian': 10.009870381339834, 'indian': 11.383242772012087, 'black': 13.554257280141982, 'white': 34.205703044617536, 'middle eastern': 17.33785031293038, 'latino hispanic': 13.509081424365213}, 'dominant_race': 'white'}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# # Def jpgs\n",
        "# jpg1 = 'dark' + '.jpg'\n",
        "# jpg2 = 'musk' + '.jpg'\n",
        "\n",
        "# # Enhance images\n",
        "# image1_path = preprocess_image(jpg1)\n",
        "# image2_path = preprocess_image(jpg2)\n",
        "\n",
        "# Verify if two images belong to the same person\n",
        "# verify_faces(jpg1, jpg2)\n",
        "\n",
        "allowed_images = [\"low2_close.jpg\"] #, \"musk_far.jpg\", \"musk_tilt.jpg\"]  # Paths to allowed images\n",
        "unknown_images = [\"low1_far_tilt.jpg\"]  # Paths to unknown images \"low1_close.jpg\", \"low2_close.jpg\", \"low1_dark.jpg\",\n",
        "\n",
        "result = verify_against_allowed(allowed_images, unknown_images, verbose=True)\n",
        "print(\"Verification Result:\", result)\n",
        "\n",
        "# Analyze a single image\n",
        "analysis = DeepFace.analyze(\"low1_far_tilt.jpg\", detector_backend=\"mtcnn\")\n",
        "print(\"Analysis: \", analysis)  # Includes demographic predictions like age, gender, emotion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKvOuTYR6Jbe"
      },
      "source": [
        "## Deploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aGTW9MG6NWj",
        "outputId": "1ed2d308-e8b9-4f8e-deee-7e2be7103c87"
      },
      "outputs": [],
      "source": [
        "# !pip install flask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "NjY3z1O36ZBu"
      },
      "outputs": [],
      "source": [
        "# !pip install ngrok\n",
        "# !ngrok http 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP8Q3VIJ6UWb",
        "outputId": "c2f0bc48-6dd1-456f-d6f8-2e7e5f672994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://10.7.184.38:5000\n",
            "Press CTRL+C to quit\n",
            "10.7.184.38 - - [21/Dec/2024 17:04:21] \"POST /upload HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary data saved to uploaded_image.jpg\n"
          ]
        }
      ],
      "source": [
        "import tempfile\n",
        "import os\n",
        "import json\n",
        "from flask import Flask, request, jsonify\n",
        "from deepface import DeepFace\n",
        "import numpy as np\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "def convert_numpy_types(obj):\n",
        "    \"\"\"Recursively convert NumPy types to native Python types.\"\"\"\n",
        "    if isinstance(obj, np.integer):  # Handles np.int32, np.int64, etc.\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):  # Handles np.float32, np.float64, etc.\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):  # Converts np.ndarray to a Python list\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):  # Recursively convert dict values\n",
        "        return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):  # Recursively convert list elements\n",
        "        return [convert_numpy_types(x) for x in obj]\n",
        "    else:\n",
        "        return obj  # If it's a normal object, return it as is\n",
        "\n",
        "@app.route('/analyze', methods=['POST'])\n",
        "def analyze_face():\n",
        "    try:\n",
        "        # Check if an image file is present in the request\n",
        "        if 'image' not in request.files:\n",
        "            return jsonify({\"error\": \"No image file provided\"}), 400\n",
        "\n",
        "        # Retrieve the image file from the request\n",
        "        image_file = request.files['image']\n",
        "\n",
        "        # Save the image to a temporary file\n",
        "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.jpg')\n",
        "        image_file.save(temp_file.name)\n",
        "        \n",
        "        # Close the temporary file to release the lock\n",
        "        temp_file.close()\n",
        "\n",
        "        # Perform face analysis using DeepFace\n",
        "        analysis = DeepFace.analyze(temp_file.name, detector_backend=\"mtcnn\")\n",
        "        print(\"image analyzed\")\n",
        "\n",
        "        # Convert NumPy types using the custom function\n",
        "        clean_analysis = convert_numpy_types(analysis)\n",
        "        \n",
        "        # Try serializing with json.dumps to catch errors\n",
        "        json_data = json.dumps({\"analysis\": clean_analysis}, default=str)  # Use str for non-serializable types\n",
        "        print(json_data)\n",
        "\n",
        "        # Delete the temporary file after use\n",
        "        os.remove(temp_file.name)\n",
        "        \n",
        "        return json_data\n",
        "    \n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/verify', methods=['POST'])\n",
        "def verify_faces():\n",
        "    try:\n",
        "        # Check if image files are present in the request\n",
        "        if 'unknown_images' not in request.files: # 'allowed_images' not in request.files or \n",
        "            return jsonify({\"error\": \"Unknown image files must be provided\"}), 400\n",
        "\n",
        "        # allowed_files = request.files.getlist('allowed_images')\n",
        "        allowed_files = allowed_images\n",
        "        unknown_files = request.files.getlist('unknown_images')\n",
        "        verbose = request.form.get('verbose', 'false').lower() == 'true'\n",
        "\n",
        "        allowed_image_paths = allowed_files\n",
        "        unknown_image_paths = []\n",
        "\n",
        "        # # Save allowed images as temporary files\n",
        "        # for image_file in allowed_files:\n",
        "        #     temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.jpg')\n",
        "        #     image_file.save(temp_file.name)\n",
        "        #     temp_file.close()\n",
        "        #     allowed_image_paths.append(temp_file.name)\n",
        "\n",
        "        # Save unknown images as temporary files\n",
        "        for image_file in unknown_files:\n",
        "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.jpg')\n",
        "            image_file.save(temp_file.name)\n",
        "            temp_file.close()\n",
        "            unknown_image_paths.append(temp_file.name)\n",
        "\n",
        "        # Perform verification using DeepFace\n",
        "        result = verify_against_allowed(allowed_image_paths, unknown_image_paths, verbose)\n",
        "\n",
        "        # Clean up temporary files\n",
        "        for path in unknown_image_paths: # allowed_image_paths + \n",
        "            try:\n",
        "                os.remove(path)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to delete temp file {path}: {e}\")\n",
        "        \n",
        "        return jsonify({\"verification_result\": result})\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# @app.route('/print', methods=['POST'])\n",
        "# def print_message():\n",
        "#     try:\n",
        "#         # Read raw data from the request body\n",
        "#         message = request.data.decode('utf-8').strip()\n",
        "\n",
        "#         # Validate the message\n",
        "#         if not message:\n",
        "#             return jsonify({\"error\": \"Empty message provided\"}), 400\n",
        "\n",
        "#         # Print the message to the terminal\n",
        "#         print(f\"Received message: {message}\")\n",
        "\n",
        "#         # Return a success response\n",
        "#         return jsonify({\"status\": \"Message printed successfully\"}), 200\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# @app.route('/upload', methods=['POST'])\n",
        "# def upload_binary():\n",
        "#     try:\n",
        "#         # Read binary data from the request body\n",
        "#         binary_data = request.data\n",
        "\n",
        "#         # Validate the data\n",
        "#         if not binary_data:\n",
        "#             return jsonify({\"error\": \"Empty binary data provided\"}), 400\n",
        "\n",
        "#         # Save the binary data to a file\n",
        "#         file_path = \"uploaded_image.jpg\"\n",
        "#         with open(file_path, \"wb\") as f:\n",
        "#             f.write(binary_data)\n",
        "\n",
        "#         print(f\"Binary data saved to {file_path}\")\n",
        "\n",
        "#         # Return a success response\n",
        "#         return jsonify({\"status\": \"Binary data received and saved successfully\"}), 200\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/', methods=['GET'])\n",
        "def health_check():\n",
        "    return jsonify({\"status\": \"OK\"})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b77rs_XzVL-C"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoFiX6CJVMv7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Directory containing the images\n",
        "image_directory = '/content'  # Update this to the correct path\n",
        "\n",
        "# Create a zip file and add only .jpg files\n",
        "with zipfile.ZipFile('/content/jpg_images.zip', 'w') as zipf:\n",
        "    for root, dirs, files in os.walk(image_directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.jpg'):\n",
        "                full_path = os.path.join(root, file)\n",
        "                zipf.write(full_path, os.path.relpath(full_path, image_directory))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gmOmkd91VWPF",
        "outputId": "c084145f-a989-48d6-9dcd-ff2413585b0c"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_2f938cd8-0357-428d-93e5-dda1d830edb1\", \"jpg_images.zip\", 71082)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the zip file containing only .jpg images\n",
        "files.download('/content/jpg_images.zip')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
